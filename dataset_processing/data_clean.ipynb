{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZHgVBb08ela"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data = pd.read_csv(\"1_CancerGov_QA.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6ilTOaqu9Xfl",
        "outputId": "458d86c0-3f5a-4e5c-bdf8-b6f68088e770"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "((583, 3), (73, 3), (73, 3))"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "data_cleaned = data.drop(['Unnamed: 0.1', 'Unnamed: 0'], axis=1)\n",
        "train_set, remaining_set = train_test_split(data_cleaned, test_size=0.2, random_state=42)\n",
        "validation_set, test_set = train_test_split(remaining_set, test_size=0.5, random_state=42)\n",
        "train_set.shape, validation_set.shape, test_set.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 70
        },
        "id": "Vvkkyixc-VjS",
        "outputId": "9fb375d2-af56-4855-f648-b7732d24a38c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/training_data.txt'"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "\n",
        "train_set_shuffled = train_set.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "validation_set_shuffled = validation_set.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "combined_data = pd.concat([train_set_shuffled, validation_set_shuffled]).reset_index(drop=True)\n",
        "\n",
        "def clean_text(text):\n",
        "    cleaned_text = text.lstrip('\\n')\n",
        "    cleaned_text = text.replace('\\t', '')\n",
        "    cleaned_text = re.sub(' +', ' ', cleaned_text)\n",
        "    cleaned_text = re.sub(' +', ' ', cleaned_text)\n",
        "    cleaned_text = re.sub(' +', ' ', cleaned_text)\n",
        "    cleaned_text = re.sub('\\n+', '\\n', cleaned_text)\n",
        "    cleaned_text = re.sub('-+', '-', cleaned_text)\n",
        "    cleaned_text = re.sub('```', '', cleaned_text)\n",
        "    cleaned_text = re.sub('\\n +', '\\n', cleaned_text)\n",
        "    cleaned_text = re.sub('\\n+', '\\n', cleaned_text)\n",
        "    cleaned_text = re.sub('\\*', '', cleaned_text)\n",
        "    cleaned_text = re.sub('-', '', cleaned_text)\n",
        "    cleaned_text = re.sub('\\n', '', cleaned_text)\n",
        "    cleaned_text = re.sub(r\"(?<=[a-zA-Z]) (?=[A-Z])\", \". \", cleaned_text)\n",
        "    return cleaned_text\n",
        "\n",
        "def remove_sentences_with_phrase(text, phrase):\n",
        "    sentences = sent_tokenize(text)\n",
        "    filtered_sentences = [sentence for sentence in sentences if phrase not in sentence]\n",
        "    return ' '.join(filtered_sentences)\n",
        "\n",
        "text_list = []\n",
        "text_content = \"\"\n",
        "phrase = \"your doctor\"\n",
        "for _, row in combined_data.iterrows():\n",
        "    row['AI Answers'] = clean_text(row['AI Answers'])\n",
        "    row['AI Answers'] = remove_sentences_with_phrase(row['AI Answers'], phrase)\n",
        "    row['Answer'] = clean_text(row['Answer'])\n",
        "    text_list.append(\"Question: {}\\nAnswer: {}\\nThis is generated by 人\\n\\n\".format(\n",
        "        row['Question'], row['Answer']))\n",
        "    text_list.append(\"Question: {}\\nAnswer: {}\\nThis is generated by 机\\n\\n\".format(\n",
        "        row['Question'], row['AI Answers']))\n",
        "\n",
        "shuffled_text_list = sorted(text_list, key=lambda x: random.random())\n",
        "for i in shuffled_text_list:\n",
        "    text_content += i\n",
        "\n",
        "text_file_path = '/content/training_data.txt'\n",
        "with open(text_file_path, 'w', encoding='utf-8') as file:\n",
        "    file.write(text_content)\n",
        "\n",
        "text_file_path"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "N-xFT3bLCZh7",
        "outputId": "589547a1-0eb6-40c0-9789-690f414bbb16"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/test_data.txt'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_text_content = \"\"\n",
        "for _, row in test_set.iterrows():\n",
        "    row['AI Answers'] = clean_text(row['AI Answers'])\n",
        "    row['Answer'] = clean_text(row['Answer'])\n",
        "    test_text_content += \"Question: {}\\nAnswer: {}\\n\\n\".format(\n",
        "        row['Question'], row['Answer'])\n",
        "    test_text_content += \"Question: {}\\nAnswer: {}\\n\\n\".format(\n",
        "        row['Question'], row['AI Answers'])\n",
        "\n",
        "test_text_file_path = '/content/test_data.txt'\n",
        "with open(test_text_file_path, 'w', encoding='utf-8') as file:\n",
        "    file.write(test_text_content)\n",
        "\n",
        "test_text_file_path"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
